
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Hand.Tremor.Landmarks\_CNN}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{a-cnn-architecture-application-for-the-recognition-of-parkinsonian-magnified-landmarks}{%
\section{A CNN architecture application for the recognition of
parkinsonian magnified
landmarks}\label{a-cnn-architecture-application-for-the-recognition-of-parkinsonian-magnified-landmarks}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
        \PY{k+kn}{from} \PY{n+nn}{tensorflow}\PY{n+nn}{.}\PY{n+nn}{python}\PY{n+nn}{.}\PY{n+nn}{framework} \PY{k}{import} \PY{n}{ops}
        \PY{k+kn}{from} \PY{n+nn}{cnn\PYZus{}utils} \PY{k}{import} \PY{o}{*} 
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
        \PY{k+kn}{from} \PY{n+nn}{keras} \PY{k}{import} \PY{n}{optimizers} 
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dense}\PY{p}{,} \PY{n}{Activation}\PY{p}{,} \PY{n}{Flatten}\PY{p}{,} \PY{n}{Dropout}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{np\PYZus{}utils}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Conv1D}\PY{p}{,} \PY{n}{GlobalAveragePooling1D}\PY{p}{,} \PY{n}{MaxPooling1D}\PY{p}{,} \PY{n}{BatchNormalization}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{initializers} \PY{k}{import} \PY{n}{he\PYZus{}normal}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{callbacks} \PY{k}{import} \PY{n}{EarlyStopping}\PY{p}{,} \PY{n}{ModelCheckpoint}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Using TensorFlow backend.

    \end{Verbatim}

    \hypertarget{data-loading}{%
\subsubsection{Data loading}\label{data-loading}}

\begin{itemize}
\tightlist
\item
  \textbf{controls.csv} contains the \textbf{magnified} average
  landmarks for control patients in \textbf{resting} position.
\item
  \textbf{parkinsonians.csv} contains the \textbf{magnified} average
  landmarks for parkinsonian patients in \textbf{resting} position.
\end{itemize}

Each average landmark was obtained from 10 finger landmarks of each
video (see figure below). There are 4 video samples per each patient: 5
control and 5 parkinson, thus yielding 20 average landmarks for each
class, i.e., 40 signals in total.\\

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{names} \PY{o}{=} \PY{p}{[}\PY{n}{c} \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{149}\PY{p}{)}\PY{p}{]}
         \PY{n}{parkinsonians} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{parkinsonians.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{names}\PY{o}{=}\PY{n}{names}\PY{p}{)}
         \PY{n}{control} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{controls.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{names}\PY{o}{=}\PY{n}{names}\PY{p}{)}
\end{Verbatim}


    \hypertarget{data-visualization}{%
\subsubsection{Data visualization}\label{data-visualization}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{axarr} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{;}
         \PY{n}{fig}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mf}{18.5}\PY{p}{,} \PY{l+m+mf}{10.5}\PY{p}{)}
         \PY{n}{axarr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{control}\PY{p}{)}\PY{p}{;}
         \PY{n}{axarr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Control Landmarks}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{axarr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{parkinsonians}\PY{p}{)}\PY{p}{;}
         \PY{n}{axarr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Parkinsonian Landmarks}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{setp}\PY{p}{(}\PY{n}{axarr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{get\PYZus{}xticklabels}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{visible}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{;}
         \PY{n}{plt}\PY{o}{.}\PY{n}{setp}\PY{p}{(}\PY{n}{axarr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{get\PYZus{}xticklabels}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{visible}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{;}
         \PY{n}{axarr}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{30}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{]}\PY{p}{)}\PY{p}{;}
         \PY{n}{axarr}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{30}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{]}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_5_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{dataset-creation}{%
\subsubsection{Dataset creation}\label{dataset-creation}}

\begin{itemize}
\tightlist
\item
  30 Landmarks of dimension 1x149 for training (15 Control, 15
  Parkinson).\\
\item
  10 Landmarks of dimension 1x149 for testing (5 Control, 5 Parkinson).
\end{itemize}

Binary classification task: Parkinson VS Control

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{X\PYZus{}train\PYZus{}orig} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{parkinsonians}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{,}\PY{n}{control}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{values}
         \PY{n}{X\PYZus{}test\PYZus{}orig} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{parkinsonians}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{,}\PY{n}{control}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{values}
         \PY{n}{Y\PYZus{}train\PYZus{}orig} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{parkinsonians}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{control}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
         \PY{n}{Y\PYZus{}test\PYZus{}orig} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{parkinsonians}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{control}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
         \PY{n}{classes} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train\PYZus{}orig}
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test\PYZus{}orig}
         \PY{n}{Y\PYZus{}train} \PY{o}{=} \PY{n}{Y\PYZus{}train\PYZus{}orig}
         \PY{n}{Y\PYZus{}test} \PY{o}{=} \PY{n}{Y\PYZus{}test\PYZus{}orig}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{number of training examples = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{number of test examples = }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X\PYZus{}train shape: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y\PYZus{}train shape: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{X\PYZus{}test shape: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Y\PYZus{}test shape: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
number of training examples = 30
number of test examples = 10
X\_train shape: (30, 149)
Y\_train shape: (30,)
X\_test shape: (10, 149)
Y\_test shape: (10,)

    \end{Verbatim}

    \hypertarget{proposed-cnn-architecture}{%
\subsubsection{Proposed CNN
architecture:}\label{proposed-cnn-architecture}}

\textbf{Description:} 1-D Convolutional model - \textbf{a. CONV 100:}
First convolutional layer with 100 filters of dimension 1x8, weight
initialization through Gaussian distribution, and Relu activation
function. - \textbf{b. MAX-POOL:} Max pooling with a 1x6 window, step=1,
no padding. - \textbf{c. CONV 50:} Second convolutional layer with 50
filters of dimension 1x4x100, weight initialization through
\emph{he\_normal} method {[}1{]}, and hyperbolic tangent activation
function. - \textbf{d. MAX-POOL:} Max pooling with a 1x3 window, step=1,
no padding. - \textbf{e. CONV 30:} Third convolutional layer with 30
filters of dimension 1x2x50, weight initialization through
\emph{he\_normal} method {[}1{]}, and hyperbolic tangent activation
function. - \textbf{f. MAX-POOL:} Max pooling with a 1x10 window,
step=1, no padding. - \textbf{g. FC 100:} Fully connected layer with 100
neurons and Relu activation function. Dropout of 0.4 to avoid model
over-training due to little data. - \textbf{h. Out:} Single neuron
output layer with sigmoid activation since it's a binary problem.

Adam optimization algorithm, binary entropy loss function and accuracy
evaluation metric. Training in 500 epochs, batch size of 16.

{[}1{]} He, Kaiming, et al. ``Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification.'' Proceedings of the
IEEE international conference on computer vision. 2015.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k}{def} \PY{n+nf}{model}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{:}
             \PY{n}{input\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{n}{name}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1Convnet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv1D}\PY{p}{(}\PY{n}{filters} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{kernel\PYZus{}initializer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{input\PYZus{}shape}\PY{o}{=}\PY{n}{input\PYZus{}shape}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling1D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv1D}\PY{p}{(}\PY{n}{filters} \PY{o}{=} \PY{l+m+mi}{50}\PY{p}{,} \PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{kernel\PYZus{}initializer}\PY{o}{=}\PY{n}{he\PYZus{}normal}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling1D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Conv1D}\PY{p}{(}\PY{n}{filters} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{,} \PY{n}{kernel\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,}\PY{n}{kernel\PYZus{}initializer}\PY{o}{=}\PY{n}{he\PYZus{}normal}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tanh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{MaxPooling1D}\PY{p}{(}\PY{n}{pool\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{strides}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.4}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sigmoid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{adam} \PY{o}{=} \PY{n}{optimizers}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{binary\PYZus{}crossentropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{adam}\PY{p}{,} \PY{n}{metrics}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{500}\PY{p}{,}\PY{n}{validation\PYZus{}data}\PY{o}{=}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{)}
             \PY{k}{return} \PY{n}{model}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k}{def} \PY{n+nf}{prepo\PYZus{}data}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{Y\PYZus{}train}\PY{p}{,}\PY{n}{Y\PYZus{}test}\PY{p}{)}\PY{p}{:}
             \PY{n}{X\PYZus{}train\PYZus{}r} \PY{o}{=}  \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
             \PY{n}{X\PYZus{}test\PYZus{}r} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{expand\PYZus{}dims}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} labels (train and test)}
             \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{Y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{Y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{Y\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{(}\PY{n}{Y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{X\PYZus{}train\PYZus{}r}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}r} \PY{p}{,} \PY{n}{y\PYZus{}test}
         
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{prepo\PYZus{}data}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{model}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{Y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{Y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
Layer (type)                 Output Shape              Param \#   
=================================================================
conv1d\_7 (Conv1D)            (None, 142, 100)          900       
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_7 (MaxPooling1 (None, 137, 100)          0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_8 (Conv1D)            (None, 134, 50)           20050     
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_8 (MaxPooling1 (None, 132, 50)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
conv1d\_9 (Conv1D)            (None, 131, 30)           3030      
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
max\_pooling1d\_9 (MaxPooling1 (None, 122, 30)           0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
flatten\_3 (Flatten)          (None, 3660)              0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_5 (Dense)              (None, 100)               366100    
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dropout\_3 (Dropout)          (None, 100)               0         
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
dense\_6 (Dense)              (None, 1)                 101       
=================================================================
Total params: 390,181
Trainable params: 390,181
Non-trainable params: 0
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_
None
Train on 30 samples, validate on 10 samples
Epoch 1/500
30/30 [==============================] - 1s 20ms/step - loss: 0.9458 - acc: 0.5333 - val\_loss: 0.7458 - val\_acc: 0.5000
Epoch 2/500
30/30 [==============================] - 0s 2ms/step - loss: 0.7264 - acc: 0.5000 - val\_loss: 0.5849 - val\_acc: 0.5000
Epoch 3/500
30/30 [==============================] - 0s 2ms/step - loss: 0.7923 - acc: 0.4000 - val\_loss: 0.3613 - val\_acc: 0.9000
Epoch 4/500
30/30 [==============================] - 0s 2ms/step - loss: 0.6511 - acc: 0.5667 - val\_loss: 0.3771 - val\_acc: 0.8000
Epoch 5/500
30/30 [==============================] - 0s 2ms/step - loss: 0.6199 - acc: 0.6333 - val\_loss: 0.5042 - val\_acc: 0.5000
Epoch 6/500
30/30 [==============================] - 0s 2ms/step - loss: 0.5728 - acc: 0.6333 - val\_loss: 0.2556 - val\_acc: 1.0000
Epoch 7/500
30/30 [==============================] - 0s 2ms/step - loss: 0.4797 - acc: 0.7333 - val\_loss: 0.2498 - val\_acc: 1.0000
Epoch 8/500
30/30 [==============================] - 0s 2ms/step - loss: 0.4557 - acc: 0.7667 - val\_loss: 0.3971 - val\_acc: 0.7000
Epoch 9/500
30/30 [==============================] - 0s 2ms/step - loss: 0.4846 - acc: 0.7333 - val\_loss: 0.4119 - val\_acc: 0.7000
Epoch 10/500
30/30 [==============================] - 0s 2ms/step - loss: 0.4171 - acc: 0.8333 - val\_loss: 0.2531 - val\_acc: 0.9000
Epoch 11/500
30/30 [==============================] - 0s 2ms/step - loss: 0.3593 - acc: 0.8000 - val\_loss: 0.2865 - val\_acc: 0.9000
Epoch 12/500
30/30 [==============================] - 0s 2ms/step - loss: 0.3193 - acc: 0.9333 - val\_loss: 0.3130 - val\_acc: 0.9000
Epoch 13/500
30/30 [==============================] - 0s 2ms/step - loss: 0.3426 - acc: 0.9000 - val\_loss: 0.4436 - val\_acc: 0.7000
Epoch 14/500
30/30 [==============================] - 0s 2ms/step - loss: 0.2997 - acc: 0.9000 - val\_loss: 0.4460 - val\_acc: 0.7000
Epoch 15/500
30/30 [==============================] - 0s 2ms/step - loss: 0.1953 - acc: 1.0000 - val\_loss: 0.2965 - val\_acc: 0.9000
Epoch 16/500
30/30 [==============================] - 0s 2ms/step - loss: 0.1621 - acc: 1.0000 - val\_loss: 0.2693 - val\_acc: 0.9000
Epoch 17/500
30/30 [==============================] - 0s 2ms/step - loss: 0.1658 - acc: 1.0000 - val\_loss: 0.2994 - val\_acc: 0.9000
Epoch 18/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0981 - acc: 1.0000 - val\_loss: 0.3669 - val\_acc: 0.9000
Epoch 19/500
30/30 [==============================] - 0s 2ms/step - loss: 0.1187 - acc: 0.9667 - val\_loss: 0.5244 - val\_acc: 0.7000
Epoch 20/500
30/30 [==============================] - 0s 2ms/step - loss: 0.1197 - acc: 1.0000 - val\_loss: 0.4971 - val\_acc: 0.7000
Epoch 21/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0624 - acc: 1.0000 - val\_loss: 0.3267 - val\_acc: 0.9000
Epoch 22/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0382 - acc: 1.0000 - val\_loss: 0.3279 - val\_acc: 0.8000
Epoch 23/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0348 - acc: 1.0000 - val\_loss: 0.3458 - val\_acc: 0.8000
Epoch 24/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0292 - acc: 1.0000 - val\_loss: 0.4298 - val\_acc: 0.9000
Epoch 25/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0224 - acc: 1.0000 - val\_loss: 0.5829 - val\_acc: 0.7000
Epoch 26/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0185 - acc: 1.0000 - val\_loss: 0.6947 - val\_acc: 0.7000
Epoch 27/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0341 - acc: 1.0000 - val\_loss: 0.5313 - val\_acc: 0.9000
Epoch 28/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0192 - acc: 1.0000 - val\_loss: 0.4651 - val\_acc: 0.8000
Epoch 29/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0082 - acc: 1.0000 - val\_loss: 0.4810 - val\_acc: 0.8000
Epoch 30/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0131 - acc: 1.0000 - val\_loss: 0.5086 - val\_acc: 0.8000
Epoch 31/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0112 - acc: 1.0000 - val\_loss: 0.5251 - val\_acc: 0.8000
Epoch 32/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0080 - acc: 1.0000 - val\_loss: 0.5524 - val\_acc: 0.8000
Epoch 33/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0038 - acc: 1.0000 - val\_loss: 0.5958 - val\_acc: 0.8000
Epoch 34/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0042 - acc: 1.0000 - val\_loss: 0.6495 - val\_acc: 0.8000
Epoch 35/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0053 - acc: 1.0000 - val\_loss: 0.6747 - val\_acc: 0.9000
Epoch 36/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0045 - acc: 1.0000 - val\_loss: 0.6767 - val\_acc: 0.9000
Epoch 37/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0045 - acc: 1.0000 - val\_loss: 0.6570 - val\_acc: 0.8000
Epoch 38/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0029 - acc: 1.0000 - val\_loss: 0.6415 - val\_acc: 0.8000
Epoch 39/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0049 - acc: 1.0000 - val\_loss: 0.6329 - val\_acc: 0.8000
Epoch 40/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0033 - acc: 1.0000 - val\_loss: 0.6445 - val\_acc: 0.8000
Epoch 41/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0083 - acc: 1.0000 - val\_loss: 0.6575 - val\_acc: 0.8000
Epoch 42/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0038 - acc: 1.0000 - val\_loss: 0.6676 - val\_acc: 0.8000
Epoch 43/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0016 - acc: 1.0000 - val\_loss: 0.6781 - val\_acc: 0.8000
Epoch 44/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0021 - acc: 1.0000 - val\_loss: 0.6911 - val\_acc: 0.8000
Epoch 45/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0023 - acc: 1.0000 - val\_loss: 0.7020 - val\_acc: 0.8000
Epoch 46/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0041 - acc: 1.0000 - val\_loss: 0.7153 - val\_acc: 0.8000
Epoch 47/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0030 - acc: 1.0000 - val\_loss: 0.7322 - val\_acc: 0.8000
Epoch 48/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0021 - acc: 1.0000 - val\_loss: 0.7478 - val\_acc: 0.8000
Epoch 49/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0018 - acc: 1.0000 - val\_loss: 0.7557 - val\_acc: 0.8000
Epoch 50/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0018 - acc: 1.0000 - val\_loss: 0.7590 - val\_acc: 0.8000
Epoch 51/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0017 - acc: 1.0000 - val\_loss: 0.7582 - val\_acc: 0.8000
Epoch 52/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 1.0000 - val\_loss: 0.7567 - val\_acc: 0.8000
Epoch 53/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 1.0000 - val\_loss: 0.7509 - val\_acc: 0.8000
Epoch 54/500
30/30 [==============================] - 0s 2ms/step - loss: 7.2867e-04 - acc: 1.0000 - val\_loss: 0.7431 - val\_acc: 0.8000
Epoch 55/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0012 - acc: 1.0000 - val\_loss: 0.7360 - val\_acc: 0.8000
Epoch 56/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0018 - acc: 1.0000 - val\_loss: 0.7327 - val\_acc: 0.8000
Epoch 57/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0018 - acc: 1.0000 - val\_loss: 0.7342 - val\_acc: 0.8000
Epoch 58/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0012 - acc: 1.0000 - val\_loss: 0.7356 - val\_acc: 0.8000
Epoch 59/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0014 - acc: 1.0000 - val\_loss: 0.7355 - val\_acc: 0.8000
Epoch 60/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0017 - acc: 1.0000 - val\_loss: 0.7403 - val\_acc: 0.8000
Epoch 61/500
30/30 [==============================] - 0s 2ms/step - loss: 5.8988e-04 - acc: 1.0000 - val\_loss: 0.7456 - val\_acc: 0.8000
Epoch 62/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0010 - acc: 1.0000 - val\_loss: 0.7499 - val\_acc: 0.8000
Epoch 63/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 1.0000 - val\_loss: 0.7533 - val\_acc: 0.8000
Epoch 64/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0010 - acc: 1.0000 - val\_loss: 0.7571 - val\_acc: 0.8000
Epoch 65/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0015 - acc: 1.0000 - val\_loss: 0.7599 - val\_acc: 0.8000
Epoch 66/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0014 - acc: 1.0000 - val\_loss: 0.7647 - val\_acc: 0.8000
Epoch 67/500
30/30 [==============================] - 0s 2ms/step - loss: 6.2920e-04 - acc: 1.0000 - val\_loss: 0.7691 - val\_acc: 0.8000
Epoch 68/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0026 - acc: 1.0000 - val\_loss: 0.7721 - val\_acc: 0.8000
Epoch 69/500
30/30 [==============================] - 0s 2ms/step - loss: 4.7981e-04 - acc: 1.0000 - val\_loss: 0.7734 - val\_acc: 0.8000
Epoch 70/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0019 - acc: 1.0000 - val\_loss: 0.7753 - val\_acc: 0.8000
Epoch 71/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0017 - acc: 1.0000 - val\_loss: 0.7770 - val\_acc: 0.8000
Epoch 72/500
30/30 [==============================] - 0s 2ms/step - loss: 3.9979e-04 - acc: 1.0000 - val\_loss: 0.7804 - val\_acc: 0.8000
Epoch 73/500
30/30 [==============================] - 0s 2ms/step - loss: 8.0683e-04 - acc: 1.0000 - val\_loss: 0.7838 - val\_acc: 0.8000
Epoch 74/500
30/30 [==============================] - 0s 2ms/step - loss: 6.8436e-04 - acc: 1.0000 - val\_loss: 0.7874 - val\_acc: 0.8000
Epoch 75/500
30/30 [==============================] - 0s 2ms/step - loss: 4.8140e-04 - acc: 1.0000 - val\_loss: 0.7904 - val\_acc: 0.8000
Epoch 76/500
30/30 [==============================] - 0s 2ms/step - loss: 2.9027e-04 - acc: 1.0000 - val\_loss: 0.7931 - val\_acc: 0.8000
Epoch 77/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0010 - acc: 1.0000 - val\_loss: 0.7954 - val\_acc: 0.8000
Epoch 78/500
30/30 [==============================] - 0s 2ms/step - loss: 7.3824e-04 - acc: 1.0000 - val\_loss: 0.7977 - val\_acc: 0.8000
Epoch 79/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0027 - acc: 1.0000 - val\_loss: 0.7980 - val\_acc: 0.8000
Epoch 80/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0012 - acc: 1.0000 - val\_loss: 0.7961 - val\_acc: 0.8000
Epoch 81/500
30/30 [==============================] - 0s 2ms/step - loss: 9.6068e-04 - acc: 1.0000 - val\_loss: 0.7938 - val\_acc: 0.8000
Epoch 82/500
30/30 [==============================] - 0s 2ms/step - loss: 4.6105e-04 - acc: 1.0000 - val\_loss: 0.7915 - val\_acc: 0.8000
Epoch 83/500
30/30 [==============================] - 0s 2ms/step - loss: 8.8007e-04 - acc: 1.0000 - val\_loss: 0.7910 - val\_acc: 0.8000
Epoch 84/500
30/30 [==============================] - 0s 2ms/step - loss: 7.7374e-04 - acc: 1.0000 - val\_loss: 0.7919 - val\_acc: 0.8000
Epoch 85/500
30/30 [==============================] - 0s 2ms/step - loss: 5.0325e-04 - acc: 1.0000 - val\_loss: 0.7934 - val\_acc: 0.8000
Epoch 86/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5272e-04 - acc: 1.0000 - val\_loss: 0.7945 - val\_acc: 0.8000
Epoch 87/500
30/30 [==============================] - 0s 2ms/step - loss: 5.2758e-04 - acc: 1.0000 - val\_loss: 0.7947 - val\_acc: 0.8000
Epoch 88/500
30/30 [==============================] - 0s 2ms/step - loss: 6.8523e-04 - acc: 1.0000 - val\_loss: 0.7958 - val\_acc: 0.8000
Epoch 89/500
30/30 [==============================] - 0s 2ms/step - loss: 8.0607e-04 - acc: 1.0000 - val\_loss: 0.8008 - val\_acc: 0.8000
Epoch 90/500
30/30 [==============================] - 0s 2ms/step - loss: 7.3570e-04 - acc: 1.0000 - val\_loss: 0.8039 - val\_acc: 0.8000
Epoch 91/500
30/30 [==============================] - 0s 2ms/step - loss: 5.6606e-04 - acc: 1.0000 - val\_loss: 0.8048 - val\_acc: 0.8000
Epoch 92/500
30/30 [==============================] - 0s 2ms/step - loss: 7.0380e-04 - acc: 1.0000 - val\_loss: 0.8037 - val\_acc: 0.8000
Epoch 93/500
30/30 [==============================] - 0s 2ms/step - loss: 4.2471e-04 - acc: 1.0000 - val\_loss: 0.8017 - val\_acc: 0.8000
Epoch 94/500
30/30 [==============================] - 0s 2ms/step - loss: 7.4021e-04 - acc: 1.0000 - val\_loss: 0.8035 - val\_acc: 0.8000
Epoch 95/500
30/30 [==============================] - 0s 2ms/step - loss: 8.0072e-04 - acc: 1.0000 - val\_loss: 0.8042 - val\_acc: 0.8000
Epoch 96/500
30/30 [==============================] - 0s 2ms/step - loss: 2.0803e-04 - acc: 1.0000 - val\_loss: 0.8044 - val\_acc: 0.8000
Epoch 97/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0012 - acc: 1.0000 - val\_loss: 0.8042 - val\_acc: 0.8000
Epoch 98/500
30/30 [==============================] - 0s 2ms/step - loss: 8.3036e-04 - acc: 1.0000 - val\_loss: 0.8038 - val\_acc: 0.8000
Epoch 99/500
30/30 [==============================] - 0s 2ms/step - loss: 4.7785e-04 - acc: 1.0000 - val\_loss: 0.8051 - val\_acc: 0.8000
Epoch 100/500
30/30 [==============================] - 0s 2ms/step - loss: 4.6404e-04 - acc: 1.0000 - val\_loss: 0.8074 - val\_acc: 0.8000
Epoch 101/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5088e-04 - acc: 1.0000 - val\_loss: 0.8096 - val\_acc: 0.8000
Epoch 102/500
30/30 [==============================] - 0s 2ms/step - loss: 2.9814e-04 - acc: 1.0000 - val\_loss: 0.8112 - val\_acc: 0.8000
Epoch 103/500
30/30 [==============================] - 0s 2ms/step - loss: 4.3460e-04 - acc: 1.0000 - val\_loss: 0.8124 - val\_acc: 0.8000
Epoch 104/500
30/30 [==============================] - 0s 2ms/step - loss: 2.7514e-04 - acc: 1.0000 - val\_loss: 0.8133 - val\_acc: 0.8000
Epoch 105/500
30/30 [==============================] - 0s 2ms/step - loss: 4.4297e-04 - acc: 1.0000 - val\_loss: 0.8139 - val\_acc: 0.8000
Epoch 106/500
30/30 [==============================] - 0s 2ms/step - loss: 3.8428e-04 - acc: 1.0000 - val\_loss: 0.8143 - val\_acc: 0.8000
Epoch 107/500
30/30 [==============================] - 0s 2ms/step - loss: 2.6545e-04 - acc: 1.0000 - val\_loss: 0.8146 - val\_acc: 0.8000
Epoch 108/500
30/30 [==============================] - 0s 2ms/step - loss: 2.0267e-04 - acc: 1.0000 - val\_loss: 0.8149 - val\_acc: 0.8000
Epoch 109/500
30/30 [==============================] - 0s 2ms/step - loss: 5.4917e-04 - acc: 1.0000 - val\_loss: 0.8157 - val\_acc: 0.8000
Epoch 110/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5675e-04 - acc: 1.0000 - val\_loss: 0.8169 - val\_acc: 0.8000
Epoch 111/500
30/30 [==============================] - 0s 2ms/step - loss: 2.6686e-04 - acc: 1.0000 - val\_loss: 0.8184 - val\_acc: 0.8000
Epoch 112/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8146e-04 - acc: 1.0000 - val\_loss: 0.8203 - val\_acc: 0.8000
Epoch 113/500
30/30 [==============================] - 0s 2ms/step - loss: 3.3679e-04 - acc: 1.0000 - val\_loss: 0.8226 - val\_acc: 0.8000
Epoch 114/500
30/30 [==============================] - 0s 2ms/step - loss: 2.6229e-04 - acc: 1.0000 - val\_loss: 0.8255 - val\_acc: 0.8000
Epoch 115/500
30/30 [==============================] - 0s 2ms/step - loss: 3.6449e-04 - acc: 1.0000 - val\_loss: 0.8279 - val\_acc: 0.8000
Epoch 116/500
30/30 [==============================] - 0s 2ms/step - loss: 2.7996e-04 - acc: 1.0000 - val\_loss: 0.8297 - val\_acc: 0.8000
Epoch 117/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8932e-04 - acc: 1.0000 - val\_loss: 0.8317 - val\_acc: 0.8000
Epoch 118/500
30/30 [==============================] - 0s 2ms/step - loss: 4.2249e-04 - acc: 1.0000 - val\_loss: 0.8346 - val\_acc: 0.8000
Epoch 119/500
30/30 [==============================] - 0s 2ms/step - loss: 0.0010 - acc: 1.0000 - val\_loss: 0.8362 - val\_acc: 0.8000
Epoch 120/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8089e-04 - acc: 1.0000 - val\_loss: 0.8358 - val\_acc: 0.8000
Epoch 121/500
30/30 [==============================] - 0s 2ms/step - loss: 3.9156e-04 - acc: 1.0000 - val\_loss: 0.8355 - val\_acc: 0.8000
Epoch 122/500
30/30 [==============================] - 0s 2ms/step - loss: 3.4295e-04 - acc: 1.0000 - val\_loss: 0.8357 - val\_acc: 0.8000
Epoch 123/500
30/30 [==============================] - 0s 2ms/step - loss: 1.3087e-04 - acc: 1.0000 - val\_loss: 0.8361 - val\_acc: 0.8000
Epoch 124/500
30/30 [==============================] - 0s 2ms/step - loss: 2.6363e-04 - acc: 1.0000 - val\_loss: 0.8370 - val\_acc: 0.8000
Epoch 125/500
30/30 [==============================] - 0s 2ms/step - loss: 2.9382e-04 - acc: 1.0000 - val\_loss: 0.8380 - val\_acc: 0.8000
Epoch 126/500
30/30 [==============================] - 0s 2ms/step - loss: 6.9397e-04 - acc: 1.0000 - val\_loss: 0.8378 - val\_acc: 0.8000
Epoch 127/500
30/30 [==============================] - 0s 2ms/step - loss: 3.5202e-04 - acc: 1.0000 - val\_loss: 0.8372 - val\_acc: 0.8000
Epoch 128/500
30/30 [==============================] - 0s 2ms/step - loss: 2.6997e-04 - acc: 1.0000 - val\_loss: 0.8371 - val\_acc: 0.8000
Epoch 129/500
30/30 [==============================] - 0s 2ms/step - loss: 9.1629e-04 - acc: 1.0000 - val\_loss: 0.8381 - val\_acc: 0.8000
Epoch 130/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5421e-04 - acc: 1.0000 - val\_loss: 0.8392 - val\_acc: 0.8000
Epoch 131/500
30/30 [==============================] - 0s 2ms/step - loss: 4.0874e-04 - acc: 1.0000 - val\_loss: 0.8398 - val\_acc: 0.8000
Epoch 132/500
30/30 [==============================] - 0s 2ms/step - loss: 1.6154e-04 - acc: 1.0000 - val\_loss: 0.8406 - val\_acc: 0.8000
Epoch 133/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2981e-04 - acc: 1.0000 - val\_loss: 0.8413 - val\_acc: 0.8000
Epoch 134/500
30/30 [==============================] - 0s 2ms/step - loss: 2.2846e-04 - acc: 1.0000 - val\_loss: 0.8419 - val\_acc: 0.8000
Epoch 135/500
30/30 [==============================] - 0s 2ms/step - loss: 8.9889e-04 - acc: 1.0000 - val\_loss: 0.8430 - val\_acc: 0.8000
Epoch 136/500
30/30 [==============================] - 0s 2ms/step - loss: 4.4904e-04 - acc: 1.0000 - val\_loss: 0.8418 - val\_acc: 0.8000
Epoch 137/500
30/30 [==============================] - 0s 2ms/step - loss: 2.1946e-04 - acc: 1.0000 - val\_loss: 0.8410 - val\_acc: 0.8000
Epoch 138/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8182e-04 - acc: 1.0000 - val\_loss: 0.8403 - val\_acc: 0.8000
Epoch 139/500
30/30 [==============================] - 0s 2ms/step - loss: 3.7159e-04 - acc: 1.0000 - val\_loss: 0.8409 - val\_acc: 0.8000
Epoch 140/500
30/30 [==============================] - 0s 2ms/step - loss: 2.4551e-04 - acc: 1.0000 - val\_loss: 0.8418 - val\_acc: 0.8000
Epoch 141/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8358e-04 - acc: 1.0000 - val\_loss: 0.8430 - val\_acc: 0.8000
Epoch 142/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5117e-04 - acc: 1.0000 - val\_loss: 0.8440 - val\_acc: 0.8000
Epoch 143/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5766e-04 - acc: 1.0000 - val\_loss: 0.8453 - val\_acc: 0.8000
Epoch 144/500
30/30 [==============================] - 0s 2ms/step - loss: 6.0013e-04 - acc: 1.0000 - val\_loss: 0.8479 - val\_acc: 0.8000
Epoch 145/500
30/30 [==============================] - 0s 2ms/step - loss: 2.2606e-04 - acc: 1.0000 - val\_loss: 0.8533 - val\_acc: 0.8000
Epoch 146/500
30/30 [==============================] - 0s 2ms/step - loss: 5.6979e-04 - acc: 1.0000 - val\_loss: 0.8564 - val\_acc: 0.8000
Epoch 147/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5422e-04 - acc: 1.0000 - val\_loss: 0.8576 - val\_acc: 0.8000
Epoch 148/500
30/30 [==============================] - 0s 2ms/step - loss: 1.8458e-04 - acc: 1.0000 - val\_loss: 0.8572 - val\_acc: 0.8000
Epoch 149/500
30/30 [==============================] - 0s 2ms/step - loss: 4.0641e-04 - acc: 1.0000 - val\_loss: 0.8576 - val\_acc: 0.8000
Epoch 150/500
30/30 [==============================] - 0s 2ms/step - loss: 5.0977e-04 - acc: 1.0000 - val\_loss: 0.8536 - val\_acc: 0.8000
Epoch 151/500
30/30 [==============================] - 0s 2ms/step - loss: 2.1909e-04 - acc: 1.0000 - val\_loss: 0.8499 - val\_acc: 0.8000
Epoch 152/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1837e-04 - acc: 1.0000 - val\_loss: 0.8476 - val\_acc: 0.8000
Epoch 153/500
30/30 [==============================] - 0s 2ms/step - loss: 1.3953e-04 - acc: 1.0000 - val\_loss: 0.8458 - val\_acc: 0.8000
Epoch 154/500
30/30 [==============================] - 0s 2ms/step - loss: 9.0944e-05 - acc: 1.0000 - val\_loss: 0.8444 - val\_acc: 0.8000
Epoch 155/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2502e-04 - acc: 1.0000 - val\_loss: 0.8434 - val\_acc: 0.8000
Epoch 156/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9398e-04 - acc: 1.0000 - val\_loss: 0.8430 - val\_acc: 0.8000
Epoch 157/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9453e-04 - acc: 1.0000 - val\_loss: 0.8430 - val\_acc: 0.8000
Epoch 158/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0199e-04 - acc: 1.0000 - val\_loss: 0.8429 - val\_acc: 0.8000
Epoch 159/500
30/30 [==============================] - 0s 2ms/step - loss: 3.5126e-04 - acc: 1.0000 - val\_loss: 0.8434 - val\_acc: 0.8000
Epoch 160/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9939e-04 - acc: 1.0000 - val\_loss: 0.8432 - val\_acc: 0.8000
Epoch 161/500
30/30 [==============================] - 0s 2ms/step - loss: 4.1724e-04 - acc: 1.0000 - val\_loss: 0.8433 - val\_acc: 0.8000
Epoch 162/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4469e-04 - acc: 1.0000 - val\_loss: 0.8442 - val\_acc: 0.8000
Epoch 163/500
30/30 [==============================] - 0s 2ms/step - loss: 2.0214e-04 - acc: 1.0000 - val\_loss: 0.8453 - val\_acc: 0.8000
Epoch 164/500
30/30 [==============================] - 0s 2ms/step - loss: 3.7012e-04 - acc: 1.0000 - val\_loss: 0.8450 - val\_acc: 0.8000
Epoch 165/500
30/30 [==============================] - 0s 2ms/step - loss: 2.6904e-04 - acc: 1.0000 - val\_loss: 0.8459 - val\_acc: 0.8000
Epoch 166/500
30/30 [==============================] - 0s 2ms/step - loss: 2.1156e-04 - acc: 1.0000 - val\_loss: 0.8475 - val\_acc: 0.8000
Epoch 167/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5839e-04 - acc: 1.0000 - val\_loss: 0.8491 - val\_acc: 0.8000
Epoch 168/500
30/30 [==============================] - 0s 2ms/step - loss: 6.6847e-05 - acc: 1.0000 - val\_loss: 0.8505 - val\_acc: 0.8000
Epoch 169/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2791e-04 - acc: 1.0000 - val\_loss: 0.8516 - val\_acc: 0.8000
Epoch 170/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1705e-04 - acc: 1.0000 - val\_loss: 0.8526 - val\_acc: 0.8000
Epoch 171/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3001e-04 - acc: 1.0000 - val\_loss: 0.8535 - val\_acc: 0.8000
Epoch 172/500
30/30 [==============================] - 0s 2ms/step - loss: 2.9280e-04 - acc: 1.0000 - val\_loss: 0.8540 - val\_acc: 0.8000
Epoch 173/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5256e-04 - acc: 1.0000 - val\_loss: 0.8545 - val\_acc: 0.8000
Epoch 174/500
30/30 [==============================] - 0s 2ms/step - loss: 2.7438e-04 - acc: 1.0000 - val\_loss: 0.8548 - val\_acc: 0.8000
Epoch 175/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4233e-04 - acc: 1.0000 - val\_loss: 0.8554 - val\_acc: 0.8000
Epoch 176/500
30/30 [==============================] - 0s 2ms/step - loss: 7.8351e-05 - acc: 1.0000 - val\_loss: 0.8560 - val\_acc: 0.8000
Epoch 177/500
30/30 [==============================] - 0s 2ms/step - loss: 3.7275e-04 - acc: 1.0000 - val\_loss: 0.8568 - val\_acc: 0.8000
Epoch 178/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2933e-04 - acc: 1.0000 - val\_loss: 0.8576 - val\_acc: 0.8000
Epoch 179/500
30/30 [==============================] - 0s 2ms/step - loss: 2.6798e-04 - acc: 1.0000 - val\_loss: 0.8592 - val\_acc: 0.8000
Epoch 180/500
30/30 [==============================] - 0s 2ms/step - loss: 7.0501e-04 - acc: 1.0000 - val\_loss: 0.8616 - val\_acc: 0.8000
Epoch 181/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1560e-04 - acc: 1.0000 - val\_loss: 0.8642 - val\_acc: 0.8000
Epoch 182/500
30/30 [==============================] - 0s 2ms/step - loss: 1.7643e-04 - acc: 1.0000 - val\_loss: 0.8668 - val\_acc: 0.8000
Epoch 183/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0317e-04 - acc: 1.0000 - val\_loss: 0.8689 - val\_acc: 0.8000
Epoch 184/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5489e-04 - acc: 1.0000 - val\_loss: 0.8708 - val\_acc: 0.8000
Epoch 185/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9129e-04 - acc: 1.0000 - val\_loss: 0.8724 - val\_acc: 0.8000
Epoch 186/500
30/30 [==============================] - 0s 2ms/step - loss: 4.1501e-04 - acc: 1.0000 - val\_loss: 0.8735 - val\_acc: 0.8000
Epoch 187/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0954e-04 - acc: 1.0000 - val\_loss: 0.8748 - val\_acc: 0.8000
Epoch 188/500
30/30 [==============================] - 0s 2ms/step - loss: 2.4369e-04 - acc: 1.0000 - val\_loss: 0.8730 - val\_acc: 0.8000
Epoch 189/500
30/30 [==============================] - 0s 2ms/step - loss: 6.6132e-05 - acc: 1.0000 - val\_loss: 0.8718 - val\_acc: 0.8000
Epoch 190/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9153e-04 - acc: 1.0000 - val\_loss: 0.8709 - val\_acc: 0.8000
Epoch 191/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9242e-04 - acc: 1.0000 - val\_loss: 0.8707 - val\_acc: 0.8000
Epoch 192/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9030e-04 - acc: 1.0000 - val\_loss: 0.8710 - val\_acc: 0.8000
Epoch 193/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9526e-04 - acc: 1.0000 - val\_loss: 0.8710 - val\_acc: 0.8000
Epoch 194/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4291e-04 - acc: 1.0000 - val\_loss: 0.8710 - val\_acc: 0.8000
Epoch 195/500
30/30 [==============================] - 0s 2ms/step - loss: 3.0632e-04 - acc: 1.0000 - val\_loss: 0.8704 - val\_acc: 0.8000
Epoch 196/500
30/30 [==============================] - 0s 2ms/step - loss: 2.2832e-04 - acc: 1.0000 - val\_loss: 0.8698 - val\_acc: 0.8000
Epoch 197/500
30/30 [==============================] - 0s 2ms/step - loss: 7.8234e-05 - acc: 1.0000 - val\_loss: 0.8695 - val\_acc: 0.8000
Epoch 198/500
30/30 [==============================] - 0s 2ms/step - loss: 3.4464e-04 - acc: 1.0000 - val\_loss: 0.8690 - val\_acc: 0.8000
Epoch 199/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0690e-04 - acc: 1.0000 - val\_loss: 0.8689 - val\_acc: 0.8000
Epoch 200/500
30/30 [==============================] - 0s 2ms/step - loss: 1.3747e-04 - acc: 1.0000 - val\_loss: 0.8689 - val\_acc: 0.8000
Epoch 201/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4167e-04 - acc: 1.0000 - val\_loss: 0.8689 - val\_acc: 0.8000
Epoch 202/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2083e-04 - acc: 1.0000 - val\_loss: 0.8693 - val\_acc: 0.8000
Epoch 203/500
30/30 [==============================] - 0s 2ms/step - loss: 4.3097e-04 - acc: 1.0000 - val\_loss: 0.8691 - val\_acc: 0.8000
Epoch 204/500
30/30 [==============================] - 0s 2ms/step - loss: 2.9059e-04 - acc: 1.0000 - val\_loss: 0.8687 - val\_acc: 0.8000
Epoch 205/500
30/30 [==============================] - 0s 2ms/step - loss: 6.7553e-05 - acc: 1.0000 - val\_loss: 0.8687 - val\_acc: 0.8000
Epoch 206/500
30/30 [==============================] - 0s 2ms/step - loss: 7.3842e-05 - acc: 1.0000 - val\_loss: 0.8686 - val\_acc: 0.8000
Epoch 207/500
30/30 [==============================] - 0s 2ms/step - loss: 5.1271e-04 - acc: 1.0000 - val\_loss: 0.8696 - val\_acc: 0.8000
Epoch 208/500
30/30 [==============================] - 0s 2ms/step - loss: 6.0245e-05 - acc: 1.0000 - val\_loss: 0.8705 - val\_acc: 0.8000
Epoch 209/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1348e-04 - acc: 1.0000 - val\_loss: 0.8708 - val\_acc: 0.8000
Epoch 210/500
30/30 [==============================] - 0s 2ms/step - loss: 2.0140e-04 - acc: 1.0000 - val\_loss: 0.8710 - val\_acc: 0.8000
Epoch 211/500
30/30 [==============================] - 0s 2ms/step - loss: 6.3848e-05 - acc: 1.0000 - val\_loss: 0.8709 - val\_acc: 0.8000
Epoch 212/500
30/30 [==============================] - 0s 2ms/step - loss: 8.6503e-05 - acc: 1.0000 - val\_loss: 0.8710 - val\_acc: 0.8000
Epoch 213/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1891e-04 - acc: 1.0000 - val\_loss: 0.8710 - val\_acc: 0.8000
Epoch 214/500
30/30 [==============================] - 0s 2ms/step - loss: 1.8351e-04 - acc: 1.0000 - val\_loss: 0.8709 - val\_acc: 0.8000
Epoch 215/500
30/30 [==============================] - 0s 2ms/step - loss: 9.1578e-05 - acc: 1.0000 - val\_loss: 0.8706 - val\_acc: 0.8000
Epoch 216/500
30/30 [==============================] - 0s 2ms/step - loss: 9.4968e-05 - acc: 1.0000 - val\_loss: 0.8701 - val\_acc: 0.8000
Epoch 217/500
30/30 [==============================] - 0s 2ms/step - loss: 5.9141e-05 - acc: 1.0000 - val\_loss: 0.8702 - val\_acc: 0.8000
Epoch 218/500
30/30 [==============================] - 0s 2ms/step - loss: 8.4575e-05 - acc: 1.0000 - val\_loss: 0.8704 - val\_acc: 0.8000
Epoch 219/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1476e-04 - acc: 1.0000 - val\_loss: 0.8705 - val\_acc: 0.8000
Epoch 220/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1867e-04 - acc: 1.0000 - val\_loss: 0.8706 - val\_acc: 0.8000
Epoch 221/500
30/30 [==============================] - 0s 2ms/step - loss: 1.6063e-04 - acc: 1.0000 - val\_loss: 0.8711 - val\_acc: 0.8000
Epoch 222/500
30/30 [==============================] - 0s 2ms/step - loss: 4.2660e-05 - acc: 1.0000 - val\_loss: 0.8717 - val\_acc: 0.8000
Epoch 223/500
30/30 [==============================] - 0s 2ms/step - loss: 7.0258e-05 - acc: 1.0000 - val\_loss: 0.8721 - val\_acc: 0.8000
Epoch 224/500
30/30 [==============================] - 0s 2ms/step - loss: 2.4421e-04 - acc: 1.0000 - val\_loss: 0.8724 - val\_acc: 0.8000
Epoch 225/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1346e-04 - acc: 1.0000 - val\_loss: 0.8730 - val\_acc: 0.8000
Epoch 226/500
30/30 [==============================] - 0s 2ms/step - loss: 7.4572e-05 - acc: 1.0000 - val\_loss: 0.8736 - val\_acc: 0.8000
Epoch 227/500
30/30 [==============================] - 0s 2ms/step - loss: 2.2882e-04 - acc: 1.0000 - val\_loss: 0.8740 - val\_acc: 0.8000
Epoch 228/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2315e-04 - acc: 1.0000 - val\_loss: 0.8743 - val\_acc: 0.8000
Epoch 229/500
30/30 [==============================] - 0s 2ms/step - loss: 5.3869e-05 - acc: 1.0000 - val\_loss: 0.8745 - val\_acc: 0.8000
Epoch 230/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5612e-04 - acc: 1.0000 - val\_loss: 0.8749 - val\_acc: 0.8000
Epoch 231/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1382e-04 - acc: 1.0000 - val\_loss: 0.8757 - val\_acc: 0.8000
Epoch 232/500
30/30 [==============================] - 0s 2ms/step - loss: 6.3874e-05 - acc: 1.0000 - val\_loss: 0.8763 - val\_acc: 0.8000
Epoch 233/500
30/30 [==============================] - 0s 2ms/step - loss: 8.2661e-05 - acc: 1.0000 - val\_loss: 0.8768 - val\_acc: 0.8000
Epoch 234/500
30/30 [==============================] - 0s 2ms/step - loss: 6.6820e-05 - acc: 1.0000 - val\_loss: 0.8769 - val\_acc: 0.8000
Epoch 235/500
30/30 [==============================] - 0s 2ms/step - loss: 4.6755e-05 - acc: 1.0000 - val\_loss: 0.8771 - val\_acc: 0.8000
Epoch 236/500
30/30 [==============================] - 0s 2ms/step - loss: 9.9719e-05 - acc: 1.0000 - val\_loss: 0.8772 - val\_acc: 0.8000
Epoch 237/500
30/30 [==============================] - 0s 2ms/step - loss: 6.0722e-05 - acc: 1.0000 - val\_loss: 0.8773 - val\_acc: 0.8000
Epoch 238/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5117e-04 - acc: 1.0000 - val\_loss: 0.8776 - val\_acc: 0.8000
Epoch 239/500
30/30 [==============================] - 0s 2ms/step - loss: 2.1621e-04 - acc: 1.0000 - val\_loss: 0.8784 - val\_acc: 0.8000
Epoch 240/500
30/30 [==============================] - 0s 1ms/step - loss: 5.1468e-05 - acc: 1.0000 - val\_loss: 0.8791 - val\_acc: 0.8000
Epoch 241/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2927e-04 - acc: 1.0000 - val\_loss: 0.8796 - val\_acc: 0.8000
Epoch 242/500
30/30 [==============================] - 0s 2ms/step - loss: 9.7990e-05 - acc: 1.0000 - val\_loss: 0.8800 - val\_acc: 0.8000
Epoch 243/500
30/30 [==============================] - 0s 2ms/step - loss: 2.6244e-04 - acc: 1.0000 - val\_loss: 0.8796 - val\_acc: 0.8000
Epoch 244/500
30/30 [==============================] - 0s 2ms/step - loss: 3.4081e-05 - acc: 1.0000 - val\_loss: 0.8789 - val\_acc: 0.8000
Epoch 245/500
30/30 [==============================] - 0s 2ms/step - loss: 6.2166e-05 - acc: 1.0000 - val\_loss: 0.8785 - val\_acc: 0.8000
Epoch 246/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1800e-04 - acc: 1.0000 - val\_loss: 0.8784 - val\_acc: 0.8000
Epoch 247/500
30/30 [==============================] - 0s 2ms/step - loss: 8.6023e-05 - acc: 1.0000 - val\_loss: 0.8791 - val\_acc: 0.8000
Epoch 248/500
30/30 [==============================] - 0s 2ms/step - loss: 9.7161e-05 - acc: 1.0000 - val\_loss: 0.8799 - val\_acc: 0.8000
Epoch 249/500
30/30 [==============================] - 0s 2ms/step - loss: 8.4121e-05 - acc: 1.0000 - val\_loss: 0.8807 - val\_acc: 0.8000
Epoch 250/500
30/30 [==============================] - 0s 2ms/step - loss: 1.6042e-04 - acc: 1.0000 - val\_loss: 0.8815 - val\_acc: 0.8000
Epoch 251/500
30/30 [==============================] - 0s 2ms/step - loss: 8.4739e-05 - acc: 1.0000 - val\_loss: 0.8828 - val\_acc: 0.8000
Epoch 252/500
30/30 [==============================] - 0s 2ms/step - loss: 3.5792e-04 - acc: 1.0000 - val\_loss: 0.8834 - val\_acc: 0.8000
Epoch 253/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4535e-04 - acc: 1.0000 - val\_loss: 0.8846 - val\_acc: 0.8000
Epoch 254/500
30/30 [==============================] - 0s 2ms/step - loss: 5.3925e-05 - acc: 1.0000 - val\_loss: 0.8868 - val\_acc: 0.8000
Epoch 255/500
30/30 [==============================] - 0s 2ms/step - loss: 4.0846e-05 - acc: 1.0000 - val\_loss: 0.8900 - val\_acc: 0.8000
Epoch 256/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0002e-04 - acc: 1.0000 - val\_loss: 0.8930 - val\_acc: 0.8000
Epoch 257/500
30/30 [==============================] - 0s 1ms/step - loss: 6.3945e-05 - acc: 1.0000 - val\_loss: 0.8952 - val\_acc: 0.8000
Epoch 258/500
30/30 [==============================] - 0s 2ms/step - loss: 1.3756e-04 - acc: 1.0000 - val\_loss: 0.8970 - val\_acc: 0.8000
Epoch 259/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1400e-04 - acc: 1.0000 - val\_loss: 0.8973 - val\_acc: 0.8000
Epoch 260/500
30/30 [==============================] - 0s 2ms/step - loss: 9.4108e-05 - acc: 1.0000 - val\_loss: 0.8975 - val\_acc: 0.8000
Epoch 261/500
30/30 [==============================] - 0s 2ms/step - loss: 9.6565e-05 - acc: 1.0000 - val\_loss: 0.8979 - val\_acc: 0.8000
Epoch 262/500
30/30 [==============================] - 0s 1ms/step - loss: 7.9657e-05 - acc: 1.0000 - val\_loss: 0.8984 - val\_acc: 0.8000
Epoch 263/500
30/30 [==============================] - 0s 2ms/step - loss: 8.1750e-05 - acc: 1.0000 - val\_loss: 0.8987 - val\_acc: 0.8000
Epoch 264/500
30/30 [==============================] - 0s 2ms/step - loss: 8.0049e-05 - acc: 1.0000 - val\_loss: 0.8989 - val\_acc: 0.8000
Epoch 265/500
30/30 [==============================] - 0s 2ms/step - loss: 7.7295e-05 - acc: 1.0000 - val\_loss: 0.8992 - val\_acc: 0.8000
Epoch 266/500
30/30 [==============================] - 0s 2ms/step - loss: 3.6557e-04 - acc: 1.0000 - val\_loss: 0.9013 - val\_acc: 0.8000
Epoch 267/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1186e-04 - acc: 1.0000 - val\_loss: 0.9033 - val\_acc: 0.8000
Epoch 268/500
30/30 [==============================] - 0s 2ms/step - loss: 4.8186e-05 - acc: 1.0000 - val\_loss: 0.9042 - val\_acc: 0.8000
Epoch 269/500
30/30 [==============================] - 0s 2ms/step - loss: 1.3706e-04 - acc: 1.0000 - val\_loss: 0.9054 - val\_acc: 0.8000
Epoch 270/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5831e-04 - acc: 1.0000 - val\_loss: 0.9056 - val\_acc: 0.8000
Epoch 271/500
30/30 [==============================] - 0s 2ms/step - loss: 5.9877e-05 - acc: 1.0000 - val\_loss: 0.9051 - val\_acc: 0.8000
Epoch 272/500
30/30 [==============================] - 0s 2ms/step - loss: 5.6001e-05 - acc: 1.0000 - val\_loss: 0.9047 - val\_acc: 0.8000
Epoch 273/500
30/30 [==============================] - 0s 2ms/step - loss: 3.7780e-05 - acc: 1.0000 - val\_loss: 0.9045 - val\_acc: 0.8000
Epoch 274/500
30/30 [==============================] - 0s 2ms/step - loss: 9.6260e-05 - acc: 1.0000 - val\_loss: 0.9043 - val\_acc: 0.8000
Epoch 275/500
30/30 [==============================] - 0s 2ms/step - loss: 3.1975e-04 - acc: 1.0000 - val\_loss: 0.9043 - val\_acc: 0.8000
Epoch 276/500
30/30 [==============================] - 0s 2ms/step - loss: 8.9380e-05 - acc: 1.0000 - val\_loss: 0.9053 - val\_acc: 0.8000
Epoch 277/500
30/30 [==============================] - 0s 2ms/step - loss: 7.1499e-05 - acc: 1.0000 - val\_loss: 0.9070 - val\_acc: 0.8000
Epoch 278/500
30/30 [==============================] - 0s 2ms/step - loss: 5.7172e-05 - acc: 1.0000 - val\_loss: 0.9085 - val\_acc: 0.8000
Epoch 279/500
30/30 [==============================] - 0s 2ms/step - loss: 7.4469e-05 - acc: 1.0000 - val\_loss: 0.9095 - val\_acc: 0.8000
Epoch 280/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4385e-04 - acc: 1.0000 - val\_loss: 0.9100 - val\_acc: 0.8000
Epoch 281/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5188e-04 - acc: 1.0000 - val\_loss: 0.9098 - val\_acc: 0.8000
Epoch 282/500
30/30 [==============================] - 0s 2ms/step - loss: 9.5115e-05 - acc: 1.0000 - val\_loss: 0.9100 - val\_acc: 0.8000
Epoch 283/500
30/30 [==============================] - 0s 2ms/step - loss: 4.6806e-05 - acc: 1.0000 - val\_loss: 0.9103 - val\_acc: 0.8000
Epoch 284/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4934e-04 - acc: 1.0000 - val\_loss: 0.9104 - val\_acc: 0.8000
Epoch 285/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2016e-04 - acc: 1.0000 - val\_loss: 0.9108 - val\_acc: 0.8000
Epoch 286/500
30/30 [==============================] - 0s 2ms/step - loss: 5.5312e-05 - acc: 1.0000 - val\_loss: 0.9111 - val\_acc: 0.8000
Epoch 287/500
30/30 [==============================] - 0s 2ms/step - loss: 5.2068e-05 - acc: 1.0000 - val\_loss: 0.9114 - val\_acc: 0.8000
Epoch 288/500
30/30 [==============================] - 0s 2ms/step - loss: 5.5179e-05 - acc: 1.0000 - val\_loss: 0.9120 - val\_acc: 0.8000
Epoch 289/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1417e-04 - acc: 1.0000 - val\_loss: 0.9127 - val\_acc: 0.8000
Epoch 290/500
30/30 [==============================] - 0s 2ms/step - loss: 8.1032e-05 - acc: 1.0000 - val\_loss: 0.9131 - val\_acc: 0.8000
Epoch 291/500
30/30 [==============================] - 0s 2ms/step - loss: 6.5703e-05 - acc: 1.0000 - val\_loss: 0.9133 - val\_acc: 0.8000
Epoch 292/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5234e-04 - acc: 1.0000 - val\_loss: 0.9131 - val\_acc: 0.8000
Epoch 293/500
30/30 [==============================] - 0s 2ms/step - loss: 3.3662e-05 - acc: 1.0000 - val\_loss: 0.9130 - val\_acc: 0.8000
Epoch 294/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4837e-04 - acc: 1.0000 - val\_loss: 0.9134 - val\_acc: 0.8000
Epoch 295/500
30/30 [==============================] - 0s 2ms/step - loss: 4.6793e-05 - acc: 1.0000 - val\_loss: 0.9142 - val\_acc: 0.8000
Epoch 296/500
30/30 [==============================] - 0s 2ms/step - loss: 3.4141e-05 - acc: 1.0000 - val\_loss: 0.9151 - val\_acc: 0.8000
Epoch 297/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8271e-04 - acc: 1.0000 - val\_loss: 0.9170 - val\_acc: 0.8000
Epoch 298/500
30/30 [==============================] - 0s 2ms/step - loss: 3.2758e-05 - acc: 1.0000 - val\_loss: 0.9198 - val\_acc: 0.8000
Epoch 299/500
30/30 [==============================] - 0s 2ms/step - loss: 3.7375e-05 - acc: 1.0000 - val\_loss: 0.9222 - val\_acc: 0.8000
Epoch 300/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1981e-04 - acc: 1.0000 - val\_loss: 0.9237 - val\_acc: 0.8000
Epoch 301/500
30/30 [==============================] - 0s 2ms/step - loss: 5.9259e-05 - acc: 1.0000 - val\_loss: 0.9245 - val\_acc: 0.8000
Epoch 302/500
30/30 [==============================] - 0s 2ms/step - loss: 3.6008e-05 - acc: 1.0000 - val\_loss: 0.9250 - val\_acc: 0.8000
Epoch 303/500
30/30 [==============================] - 0s 2ms/step - loss: 9.3453e-05 - acc: 1.0000 - val\_loss: 0.9250 - val\_acc: 0.8000
Epoch 304/500
30/30 [==============================] - 0s 2ms/step - loss: 9.5486e-05 - acc: 1.0000 - val\_loss: 0.9246 - val\_acc: 0.8000
Epoch 305/500
30/30 [==============================] - 0s 2ms/step - loss: 9.5147e-05 - acc: 1.0000 - val\_loss: 0.9246 - val\_acc: 0.8000
Epoch 306/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8642e-05 - acc: 1.0000 - val\_loss: 0.9248 - val\_acc: 0.8000
Epoch 307/500
30/30 [==============================] - 0s 2ms/step - loss: 5.6020e-05 - acc: 1.0000 - val\_loss: 0.9246 - val\_acc: 0.8000
Epoch 308/500
30/30 [==============================] - 0s 2ms/step - loss: 7.9496e-05 - acc: 1.0000 - val\_loss: 0.9243 - val\_acc: 0.8000
Epoch 309/500
30/30 [==============================] - 0s 1ms/step - loss: 3.9498e-05 - acc: 1.0000 - val\_loss: 0.9241 - val\_acc: 0.8000
Epoch 310/500
30/30 [==============================] - 0s 2ms/step - loss: 9.9616e-05 - acc: 1.0000 - val\_loss: 0.9241 - val\_acc: 0.8000
Epoch 311/500
30/30 [==============================] - 0s 2ms/step - loss: 6.3999e-05 - acc: 1.0000 - val\_loss: 0.9242 - val\_acc: 0.8000
Epoch 312/500
30/30 [==============================] - 0s 2ms/step - loss: 8.4275e-05 - acc: 1.0000 - val\_loss: 0.9241 - val\_acc: 0.8000
Epoch 313/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2814e-04 - acc: 1.0000 - val\_loss: 0.9240 - val\_acc: 0.8000
Epoch 314/500
30/30 [==============================] - 0s 2ms/step - loss: 7.1164e-05 - acc: 1.0000 - val\_loss: 0.9234 - val\_acc: 0.8000
Epoch 315/500
30/30 [==============================] - 0s 2ms/step - loss: 6.0984e-05 - acc: 1.0000 - val\_loss: 0.9228 - val\_acc: 0.8000
Epoch 316/500
30/30 [==============================] - 0s 2ms/step - loss: 3.5453e-05 - acc: 1.0000 - val\_loss: 0.9222 - val\_acc: 0.8000
Epoch 317/500
30/30 [==============================] - 0s 2ms/step - loss: 6.2323e-05 - acc: 1.0000 - val\_loss: 0.9219 - val\_acc: 0.8000
Epoch 318/500
30/30 [==============================] - 0s 2ms/step - loss: 8.3092e-05 - acc: 1.0000 - val\_loss: 0.9218 - val\_acc: 0.8000
Epoch 319/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3440e-05 - acc: 1.0000 - val\_loss: 0.9219 - val\_acc: 0.8000
Epoch 320/500
30/30 [==============================] - 0s 2ms/step - loss: 3.2780e-05 - acc: 1.0000 - val\_loss: 0.9220 - val\_acc: 0.8000
Epoch 321/500
30/30 [==============================] - 0s 2ms/step - loss: 9.0222e-05 - acc: 1.0000 - val\_loss: 0.9224 - val\_acc: 0.8000
Epoch 322/500
30/30 [==============================] - 0s 2ms/step - loss: 3.5320e-05 - acc: 1.0000 - val\_loss: 0.9228 - val\_acc: 0.8000
Epoch 323/500
30/30 [==============================] - 0s 2ms/step - loss: 6.6436e-05 - acc: 1.0000 - val\_loss: 0.9232 - val\_acc: 0.8000
Epoch 324/500
30/30 [==============================] - 0s 2ms/step - loss: 3.9041e-05 - acc: 1.0000 - val\_loss: 0.9236 - val\_acc: 0.8000
Epoch 325/500
30/30 [==============================] - 0s 2ms/step - loss: 9.4280e-05 - acc: 1.0000 - val\_loss: 0.9238 - val\_acc: 0.8000
Epoch 326/500
30/30 [==============================] - 0s 2ms/step - loss: 8.1971e-05 - acc: 1.0000 - val\_loss: 0.9240 - val\_acc: 0.8000
Epoch 327/500
30/30 [==============================] - 0s 2ms/step - loss: 9.2643e-05 - acc: 1.0000 - val\_loss: 0.9242 - val\_acc: 0.8000
Epoch 328/500
30/30 [==============================] - 0s 2ms/step - loss: 4.1755e-05 - acc: 1.0000 - val\_loss: 0.9242 - val\_acc: 0.8000
Epoch 329/500
30/30 [==============================] - 0s 2ms/step - loss: 6.0135e-05 - acc: 1.0000 - val\_loss: 0.9239 - val\_acc: 0.8000
Epoch 330/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0001e-04 - acc: 1.0000 - val\_loss: 0.9248 - val\_acc: 0.8000
Epoch 331/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3437e-05 - acc: 1.0000 - val\_loss: 0.9255 - val\_acc: 0.8000
Epoch 332/500
30/30 [==============================] - 0s 2ms/step - loss: 6.8733e-05 - acc: 1.0000 - val\_loss: 0.9260 - val\_acc: 0.8000
Epoch 333/500
30/30 [==============================] - 0s 2ms/step - loss: 5.8812e-05 - acc: 1.0000 - val\_loss: 0.9263 - val\_acc: 0.8000
Epoch 334/500
30/30 [==============================] - 0s 2ms/step - loss: 7.3389e-05 - acc: 1.0000 - val\_loss: 0.9265 - val\_acc: 0.8000
Epoch 335/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3632e-05 - acc: 1.0000 - val\_loss: 0.9266 - val\_acc: 0.8000
Epoch 336/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8213e-05 - acc: 1.0000 - val\_loss: 0.9267 - val\_acc: 0.8000
Epoch 337/500
30/30 [==============================] - 0s 2ms/step - loss: 1.6166e-04 - acc: 1.0000 - val\_loss: 0.9269 - val\_acc: 0.8000
Epoch 338/500
30/30 [==============================] - 0s 2ms/step - loss: 8.3121e-05 - acc: 1.0000 - val\_loss: 0.9270 - val\_acc: 0.8000
Epoch 339/500
30/30 [==============================] - 0s 2ms/step - loss: 5.0104e-05 - acc: 1.0000 - val\_loss: 0.9273 - val\_acc: 0.8000
Epoch 340/500
30/30 [==============================] - 0s 2ms/step - loss: 4.1048e-05 - acc: 1.0000 - val\_loss: 0.9274 - val\_acc: 0.8000
Epoch 341/500
30/30 [==============================] - 0s 2ms/step - loss: 1.7193e-04 - acc: 1.0000 - val\_loss: 0.9289 - val\_acc: 0.8000
Epoch 342/500
30/30 [==============================] - 0s 2ms/step - loss: 8.0949e-05 - acc: 1.0000 - val\_loss: 0.9303 - val\_acc: 0.8000
Epoch 343/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1505e-04 - acc: 1.0000 - val\_loss: 0.9311 - val\_acc: 0.8000
Epoch 344/500
30/30 [==============================] - 0s 2ms/step - loss: 3.0032e-04 - acc: 1.0000 - val\_loss: 0.9311 - val\_acc: 0.8000
Epoch 345/500
30/30 [==============================] - 0s 2ms/step - loss: 3.6244e-05 - acc: 1.0000 - val\_loss: 0.9311 - val\_acc: 0.8000
Epoch 346/500
30/30 [==============================] - 0s 2ms/step - loss: 1.8782e-04 - acc: 1.0000 - val\_loss: 0.9299 - val\_acc: 0.8000
Epoch 347/500
30/30 [==============================] - 0s 2ms/step - loss: 7.4579e-05 - acc: 1.0000 - val\_loss: 0.9275 - val\_acc: 0.8000
Epoch 348/500
30/30 [==============================] - 0s 2ms/step - loss: 1.6288e-04 - acc: 1.0000 - val\_loss: 0.9261 - val\_acc: 0.8000
Epoch 349/500
30/30 [==============================] - 0s 2ms/step - loss: 6.1485e-05 - acc: 1.0000 - val\_loss: 0.9254 - val\_acc: 0.8000
Epoch 350/500
30/30 [==============================] - 0s 2ms/step - loss: 2.4802e-05 - acc: 1.0000 - val\_loss: 0.9258 - val\_acc: 0.8000
Epoch 351/500
30/30 [==============================] - 0s 2ms/step - loss: 2.7513e-05 - acc: 1.0000 - val\_loss: 0.9265 - val\_acc: 0.8000
Epoch 352/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4862e-04 - acc: 1.0000 - val\_loss: 0.9278 - val\_acc: 0.8000
Epoch 353/500
30/30 [==============================] - 0s 2ms/step - loss: 1.9962e-05 - acc: 1.0000 - val\_loss: 0.9300 - val\_acc: 0.8000
Epoch 354/500
30/30 [==============================] - 0s 2ms/step - loss: 8.9068e-05 - acc: 1.0000 - val\_loss: 0.9309 - val\_acc: 0.8000
Epoch 355/500
30/30 [==============================] - 0s 2ms/step - loss: 4.3266e-05 - acc: 1.0000 - val\_loss: 0.9315 - val\_acc: 0.8000
Epoch 356/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0829e-04 - acc: 1.0000 - val\_loss: 0.9313 - val\_acc: 0.8000
Epoch 357/500
30/30 [==============================] - 0s 2ms/step - loss: 3.8563e-05 - acc: 1.0000 - val\_loss: 0.9311 - val\_acc: 0.8000
Epoch 358/500
30/30 [==============================] - 0s 1ms/step - loss: 8.8249e-05 - acc: 1.0000 - val\_loss: 0.9312 - val\_acc: 0.8000
Epoch 359/500
30/30 [==============================] - 0s 2ms/step - loss: 8.0709e-05 - acc: 1.0000 - val\_loss: 0.9314 - val\_acc: 0.8000
Epoch 360/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1829e-04 - acc: 1.0000 - val\_loss: 0.9309 - val\_acc: 0.8000
Epoch 361/500
30/30 [==============================] - 0s 2ms/step - loss: 8.5545e-05 - acc: 1.0000 - val\_loss: 0.9307 - val\_acc: 0.8000
Epoch 362/500
30/30 [==============================] - 0s 2ms/step - loss: 4.8275e-05 - acc: 1.0000 - val\_loss: 0.9310 - val\_acc: 0.8000
Epoch 363/500
30/30 [==============================] - 0s 2ms/step - loss: 4.2506e-05 - acc: 1.0000 - val\_loss: 0.9321 - val\_acc: 0.8000
Epoch 364/500
30/30 [==============================] - 0s 2ms/step - loss: 4.8004e-04 - acc: 1.0000 - val\_loss: 0.9347 - val\_acc: 0.8000
Epoch 365/500
30/30 [==============================] - 0s 2ms/step - loss: 8.8428e-05 - acc: 1.0000 - val\_loss: 0.9380 - val\_acc: 0.8000
Epoch 366/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0595e-04 - acc: 1.0000 - val\_loss: 0.9416 - val\_acc: 0.8000
Epoch 367/500
30/30 [==============================] - 0s 2ms/step - loss: 5.9715e-05 - acc: 1.0000 - val\_loss: 0.9441 - val\_acc: 0.8000
Epoch 368/500
30/30 [==============================] - 0s 2ms/step - loss: 1.7657e-04 - acc: 1.0000 - val\_loss: 0.9437 - val\_acc: 0.8000
Epoch 369/500
30/30 [==============================] - 0s 1ms/step - loss: 9.1962e-05 - acc: 1.0000 - val\_loss: 0.9429 - val\_acc: 0.8000
Epoch 370/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0063e-04 - acc: 1.0000 - val\_loss: 0.9433 - val\_acc: 0.8000
Epoch 371/500
30/30 [==============================] - 0s 2ms/step - loss: 2.4943e-04 - acc: 1.0000 - val\_loss: 0.9438 - val\_acc: 0.8000
Epoch 372/500
30/30 [==============================] - 0s 2ms/step - loss: 3.2052e-04 - acc: 1.0000 - val\_loss: 0.9435 - val\_acc: 0.8000
Epoch 373/500
30/30 [==============================] - 0s 2ms/step - loss: 5.3496e-05 - acc: 1.0000 - val\_loss: 0.9428 - val\_acc: 0.8000
Epoch 374/500
30/30 [==============================] - 0s 2ms/step - loss: 4.6956e-05 - acc: 1.0000 - val\_loss: 0.9427 - val\_acc: 0.8000
Epoch 375/500
30/30 [==============================] - 0s 2ms/step - loss: 4.0742e-05 - acc: 1.0000 - val\_loss: 0.9427 - val\_acc: 0.8000
Epoch 376/500
30/30 [==============================] - 0s 2ms/step - loss: 1.3467e-04 - acc: 1.0000 - val\_loss: 0.9426 - val\_acc: 0.8000
Epoch 377/500
30/30 [==============================] - 0s 2ms/step - loss: 3.6164e-05 - acc: 1.0000 - val\_loss: 0.9425 - val\_acc: 0.8000
Epoch 378/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8627e-05 - acc: 1.0000 - val\_loss: 0.9424 - val\_acc: 0.8000
Epoch 379/500
30/30 [==============================] - 0s 2ms/step - loss: 6.1508e-05 - acc: 1.0000 - val\_loss: 0.9422 - val\_acc: 0.8000
Epoch 380/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5668e-05 - acc: 1.0000 - val\_loss: 0.9421 - val\_acc: 0.8000
Epoch 381/500
30/30 [==============================] - 0s 2ms/step - loss: 3.8008e-05 - acc: 1.0000 - val\_loss: 0.9420 - val\_acc: 0.8000
Epoch 382/500
30/30 [==============================] - 0s 2ms/step - loss: 2.9455e-05 - acc: 1.0000 - val\_loss: 0.9420 - val\_acc: 0.8000
Epoch 383/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3111e-05 - acc: 1.0000 - val\_loss: 0.9419 - val\_acc: 0.8000
Epoch 384/500
30/30 [==============================] - 0s 2ms/step - loss: 4.4251e-05 - acc: 1.0000 - val\_loss: 0.9422 - val\_acc: 0.8000
Epoch 385/500
30/30 [==============================] - 0s 2ms/step - loss: 3.9996e-05 - acc: 1.0000 - val\_loss: 0.9425 - val\_acc: 0.8000
Epoch 386/500
30/30 [==============================] - 0s 2ms/step - loss: 6.8959e-05 - acc: 1.0000 - val\_loss: 0.9430 - val\_acc: 0.8000
Epoch 387/500
30/30 [==============================] - 0s 2ms/step - loss: 4.4430e-05 - acc: 1.0000 - val\_loss: 0.9435 - val\_acc: 0.8000
Epoch 388/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0105e-04 - acc: 1.0000 - val\_loss: 0.9440 - val\_acc: 0.8000
Epoch 389/500
30/30 [==============================] - 0s 2ms/step - loss: 3.4893e-05 - acc: 1.0000 - val\_loss: 0.9444 - val\_acc: 0.8000
Epoch 390/500
30/30 [==============================] - 0s 2ms/step - loss: 8.6319e-05 - acc: 1.0000 - val\_loss: 0.9446 - val\_acc: 0.8000
Epoch 391/500
30/30 [==============================] - 0s 2ms/step - loss: 3.0997e-05 - acc: 1.0000 - val\_loss: 0.9448 - val\_acc: 0.8000
Epoch 392/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0099e-04 - acc: 1.0000 - val\_loss: 0.9462 - val\_acc: 0.8000
Epoch 393/500
30/30 [==============================] - 0s 2ms/step - loss: 3.4583e-05 - acc: 1.0000 - val\_loss: 0.9485 - val\_acc: 0.8000
Epoch 394/500
30/30 [==============================] - 0s 2ms/step - loss: 2.0050e-05 - acc: 1.0000 - val\_loss: 0.9503 - val\_acc: 0.8000
Epoch 395/500
30/30 [==============================] - 0s 2ms/step - loss: 7.6288e-04 - acc: 1.0000 - val\_loss: 0.9411 - val\_acc: 0.8000
Epoch 396/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3456e-05 - acc: 1.0000 - val\_loss: 0.9412 - val\_acc: 0.8000
Epoch 397/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3389e-05 - acc: 1.0000 - val\_loss: 0.9484 - val\_acc: 0.8000
Epoch 398/500
30/30 [==============================] - 0s 2ms/step - loss: 3.5458e-05 - acc: 1.0000 - val\_loss: 0.9578 - val\_acc: 0.8000
Epoch 399/500
30/30 [==============================] - 0s 2ms/step - loss: 5.9838e-05 - acc: 1.0000 - val\_loss: 0.9646 - val\_acc: 0.8000
Epoch 400/500
30/30 [==============================] - 0s 2ms/step - loss: 6.4304e-05 - acc: 1.0000 - val\_loss: 0.9672 - val\_acc: 0.8000
Epoch 401/500
30/30 [==============================] - 0s 2ms/step - loss: 3.3266e-05 - acc: 1.0000 - val\_loss: 0.9681 - val\_acc: 0.7000
Epoch 402/500
30/30 [==============================] - 0s 2ms/step - loss: 5.9234e-05 - acc: 1.0000 - val\_loss: 0.9660 - val\_acc: 0.7000
Epoch 403/500
30/30 [==============================] - 0s 2ms/step - loss: 4.8360e-05 - acc: 1.0000 - val\_loss: 0.9653 - val\_acc: 0.8000
Epoch 404/500
30/30 [==============================] - 0s 2ms/step - loss: 3.9015e-05 - acc: 1.0000 - val\_loss: 0.9641 - val\_acc: 0.9000
Epoch 405/500
30/30 [==============================] - 0s 2ms/step - loss: 3.5556e-05 - acc: 1.0000 - val\_loss: 0.9621 - val\_acc: 0.9000
Epoch 406/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1133e-04 - acc: 1.0000 - val\_loss: 0.9546 - val\_acc: 0.8000
Epoch 407/500
30/30 [==============================] - 0s 2ms/step - loss: 7.1998e-05 - acc: 1.0000 - val\_loss: 0.9454 - val\_acc: 0.8000
Epoch 408/500
30/30 [==============================] - 0s 2ms/step - loss: 3.7121e-05 - acc: 1.0000 - val\_loss: 0.9379 - val\_acc: 0.8000
Epoch 409/500
30/30 [==============================] - 0s 2ms/step - loss: 6.3061e-05 - acc: 1.0000 - val\_loss: 0.9326 - val\_acc: 0.8000
Epoch 410/500
30/30 [==============================] - 0s 2ms/step - loss: 4.6771e-05 - acc: 1.0000 - val\_loss: 0.9288 - val\_acc: 0.8000
Epoch 411/500
30/30 [==============================] - 0s 2ms/step - loss: 7.1963e-05 - acc: 1.0000 - val\_loss: 0.9262 - val\_acc: 0.8000
Epoch 412/500
30/30 [==============================] - 0s 1ms/step - loss: 3.6661e-05 - acc: 1.0000 - val\_loss: 0.9244 - val\_acc: 0.8000
Epoch 413/500
30/30 [==============================] - 0s 2ms/step - loss: 4.2316e-05 - acc: 1.0000 - val\_loss: 0.9230 - val\_acc: 0.8000
Epoch 414/500
30/30 [==============================] - 0s 2ms/step - loss: 1.3901e-04 - acc: 1.0000 - val\_loss: 0.9222 - val\_acc: 0.8000
Epoch 415/500
30/30 [==============================] - 0s 2ms/step - loss: 3.0592e-05 - acc: 1.0000 - val\_loss: 0.9234 - val\_acc: 0.8000
Epoch 416/500
30/30 [==============================] - 0s 1ms/step - loss: 5.4887e-05 - acc: 1.0000 - val\_loss: 0.9252 - val\_acc: 0.8000
Epoch 417/500
30/30 [==============================] - 0s 2ms/step - loss: 6.3873e-05 - acc: 1.0000 - val\_loss: 0.9275 - val\_acc: 0.8000
Epoch 418/500
30/30 [==============================] - 0s 1ms/step - loss: 2.9352e-05 - acc: 1.0000 - val\_loss: 0.9300 - val\_acc: 0.8000
Epoch 419/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0221e-04 - acc: 1.0000 - val\_loss: 0.9317 - val\_acc: 0.8000
Epoch 420/500
30/30 [==============================] - 0s 2ms/step - loss: 9.1307e-06 - acc: 1.0000 - val\_loss: 0.9332 - val\_acc: 0.8000
Epoch 421/500
30/30 [==============================] - 0s 2ms/step - loss: 1.5896e-05 - acc: 1.0000 - val\_loss: 0.9345 - val\_acc: 0.8000
Epoch 422/500
30/30 [==============================] - 0s 1ms/step - loss: 7.5470e-05 - acc: 1.0000 - val\_loss: 0.9356 - val\_acc: 0.8000
Epoch 423/500
30/30 [==============================] - 0s 2ms/step - loss: 4.6186e-05 - acc: 1.0000 - val\_loss: 0.9366 - val\_acc: 0.8000
Epoch 424/500
30/30 [==============================] - 0s 1ms/step - loss: 3.2315e-05 - acc: 1.0000 - val\_loss: 0.9376 - val\_acc: 0.8000
Epoch 425/500
30/30 [==============================] - 0s 2ms/step - loss: 3.0008e-05 - acc: 1.0000 - val\_loss: 0.9384 - val\_acc: 0.8000
Epoch 426/500
30/30 [==============================] - 0s 1ms/step - loss: 4.5059e-05 - acc: 1.0000 - val\_loss: 0.9387 - val\_acc: 0.8000
Epoch 427/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5421e-05 - acc: 1.0000 - val\_loss: 0.9388 - val\_acc: 0.8000
Epoch 428/500
30/30 [==============================] - 0s 2ms/step - loss: 2.4630e-05 - acc: 1.0000 - val\_loss: 0.9388 - val\_acc: 0.8000
Epoch 429/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3452e-05 - acc: 1.0000 - val\_loss: 0.9386 - val\_acc: 0.8000
Epoch 430/500
30/30 [==============================] - 0s 2ms/step - loss: 3.6236e-05 - acc: 1.0000 - val\_loss: 0.9379 - val\_acc: 0.8000
Epoch 431/500
30/30 [==============================] - 0s 2ms/step - loss: 3.2175e-05 - acc: 1.0000 - val\_loss: 0.9368 - val\_acc: 0.8000
Epoch 432/500
30/30 [==============================] - 0s 2ms/step - loss: 1.3715e-04 - acc: 1.0000 - val\_loss: 0.9358 - val\_acc: 0.8000
Epoch 433/500
30/30 [==============================] - 0s 2ms/step - loss: 8.3550e-05 - acc: 1.0000 - val\_loss: 0.9357 - val\_acc: 0.8000
Epoch 434/500
30/30 [==============================] - 0s 2ms/step - loss: 1.7313e-05 - acc: 1.0000 - val\_loss: 0.9361 - val\_acc: 0.8000
Epoch 435/500
30/30 [==============================] - 0s 2ms/step - loss: 6.2747e-05 - acc: 1.0000 - val\_loss: 0.9364 - val\_acc: 0.8000
Epoch 436/500
30/30 [==============================] - 0s 1ms/step - loss: 2.9011e-04 - acc: 1.0000 - val\_loss: 0.9380 - val\_acc: 0.8000
Epoch 437/500
30/30 [==============================] - 0s 2ms/step - loss: 1.7917e-05 - acc: 1.0000 - val\_loss: 0.9412 - val\_acc: 0.8000
Epoch 438/500
30/30 [==============================] - 0s 2ms/step - loss: 3.5185e-05 - acc: 1.0000 - val\_loss: 0.9435 - val\_acc: 0.8000
Epoch 439/500
30/30 [==============================] - 0s 2ms/step - loss: 4.2701e-05 - acc: 1.0000 - val\_loss: 0.9453 - val\_acc: 0.8000
Epoch 440/500
30/30 [==============================] - 0s 2ms/step - loss: 8.2429e-06 - acc: 1.0000 - val\_loss: 0.9467 - val\_acc: 0.8000
Epoch 441/500
30/30 [==============================] - 0s 2ms/step - loss: 1.8891e-05 - acc: 1.0000 - val\_loss: 0.9477 - val\_acc: 0.8000
Epoch 442/500
30/30 [==============================] - 0s 1ms/step - loss: 8.5463e-06 - acc: 1.0000 - val\_loss: 0.9484 - val\_acc: 0.8000
Epoch 443/500
30/30 [==============================] - 0s 2ms/step - loss: 2.1039e-05 - acc: 1.0000 - val\_loss: 0.9492 - val\_acc: 0.8000
Epoch 444/500
30/30 [==============================] - 0s 2ms/step - loss: 3.7820e-05 - acc: 1.0000 - val\_loss: 0.9499 - val\_acc: 0.8000
Epoch 445/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2390e-05 - acc: 1.0000 - val\_loss: 0.9504 - val\_acc: 0.8000
Epoch 446/500
30/30 [==============================] - 0s 1ms/step - loss: 5.7914e-05 - acc: 1.0000 - val\_loss: 0.9507 - val\_acc: 0.8000
Epoch 447/500
30/30 [==============================] - 0s 2ms/step - loss: 4.0730e-05 - acc: 1.0000 - val\_loss: 0.9510 - val\_acc: 0.8000
Epoch 448/500
30/30 [==============================] - 0s 2ms/step - loss: 3.6032e-05 - acc: 1.0000 - val\_loss: 0.9512 - val\_acc: 0.8000
Epoch 449/500
30/30 [==============================] - 0s 1ms/step - loss: 4.2932e-05 - acc: 1.0000 - val\_loss: 0.9512 - val\_acc: 0.8000
Epoch 450/500
30/30 [==============================] - 0s 2ms/step - loss: 2.7738e-05 - acc: 1.0000 - val\_loss: 0.9511 - val\_acc: 0.8000
Epoch 451/500
30/30 [==============================] - 0s 2ms/step - loss: 9.8808e-06 - acc: 1.0000 - val\_loss: 0.9510 - val\_acc: 0.8000
Epoch 452/500
30/30 [==============================] - 0s 2ms/step - loss: 5.4140e-05 - acc: 1.0000 - val\_loss: 0.9509 - val\_acc: 0.8000
Epoch 453/500
30/30 [==============================] - 0s 2ms/step - loss: 5.0804e-05 - acc: 1.0000 - val\_loss: 0.9504 - val\_acc: 0.8000
Epoch 454/500
30/30 [==============================] - 0s 1ms/step - loss: 1.0827e-04 - acc: 1.0000 - val\_loss: 0.9491 - val\_acc: 0.8000
Epoch 455/500
30/30 [==============================] - 0s 1ms/step - loss: 2.6351e-05 - acc: 1.0000 - val\_loss: 0.9486 - val\_acc: 0.8000
Epoch 456/500
30/30 [==============================] - 0s 2ms/step - loss: 8.4819e-05 - acc: 1.0000 - val\_loss: 0.9488 - val\_acc: 0.8000
Epoch 457/500
30/30 [==============================] - 0s 1ms/step - loss: 2.4161e-05 - acc: 1.0000 - val\_loss: 0.9490 - val\_acc: 0.8000
Epoch 458/500
30/30 [==============================] - 0s 2ms/step - loss: 3.4936e-05 - acc: 1.0000 - val\_loss: 0.9493 - val\_acc: 0.8000
Epoch 459/500
30/30 [==============================] - 0s 2ms/step - loss: 3.1355e-05 - acc: 1.0000 - val\_loss: 0.9496 - val\_acc: 0.8000
Epoch 460/500
30/30 [==============================] - 0s 1ms/step - loss: 3.1218e-05 - acc: 1.0000 - val\_loss: 0.9499 - val\_acc: 0.8000
Epoch 461/500
30/30 [==============================] - 0s 1ms/step - loss: 8.8259e-06 - acc: 1.0000 - val\_loss: 0.9501 - val\_acc: 0.8000
Epoch 462/500
30/30 [==============================] - 0s 2ms/step - loss: 1.8675e-05 - acc: 1.0000 - val\_loss: 0.9504 - val\_acc: 0.8000
Epoch 463/500
30/30 [==============================] - 0s 2ms/step - loss: 2.5407e-05 - acc: 1.0000 - val\_loss: 0.9506 - val\_acc: 0.8000
Epoch 464/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0029e-05 - acc: 1.0000 - val\_loss: 0.9508 - val\_acc: 0.8000
Epoch 465/500
30/30 [==============================] - 0s 2ms/step - loss: 5.0685e-05 - acc: 1.0000 - val\_loss: 0.9512 - val\_acc: 0.8000
Epoch 466/500
30/30 [==============================] - 0s 2ms/step - loss: 1.7715e-05 - acc: 1.0000 - val\_loss: 0.9515 - val\_acc: 0.8000
Epoch 467/500
30/30 [==============================] - 0s 2ms/step - loss: 1.2902e-05 - acc: 1.0000 - val\_loss: 0.9518 - val\_acc: 0.8000
Epoch 468/500
30/30 [==============================] - 0s 1ms/step - loss: 5.4025e-05 - acc: 1.0000 - val\_loss: 0.9523 - val\_acc: 0.8000
Epoch 469/500
30/30 [==============================] - 0s 2ms/step - loss: 8.4107e-05 - acc: 1.0000 - val\_loss: 0.9530 - val\_acc: 0.8000
Epoch 470/500
30/30 [==============================] - 0s 1ms/step - loss: 3.3536e-05 - acc: 1.0000 - val\_loss: 0.9534 - val\_acc: 0.8000
Epoch 471/500
30/30 [==============================] - 0s 2ms/step - loss: 4.7554e-05 - acc: 1.0000 - val\_loss: 0.9540 - val\_acc: 0.8000
Epoch 472/500
30/30 [==============================] - 0s 1ms/step - loss: 5.1223e-05 - acc: 1.0000 - val\_loss: 0.9546 - val\_acc: 0.8000
Epoch 473/500
30/30 [==============================] - 0s 1ms/step - loss: 1.5398e-05 - acc: 1.0000 - val\_loss: 0.9554 - val\_acc: 0.8000
Epoch 474/500
30/30 [==============================] - 0s 1ms/step - loss: 1.5816e-05 - acc: 1.0000 - val\_loss: 0.9563 - val\_acc: 0.8000
Epoch 475/500
30/30 [==============================] - 0s 2ms/step - loss: 4.8602e-05 - acc: 1.0000 - val\_loss: 0.9573 - val\_acc: 0.8000
Epoch 476/500
30/30 [==============================] - 0s 1ms/step - loss: 3.5923e-05 - acc: 1.0000 - val\_loss: 0.9583 - val\_acc: 0.8000
Epoch 477/500
30/30 [==============================] - 0s 2ms/step - loss: 2.8929e-05 - acc: 1.0000 - val\_loss: 0.9596 - val\_acc: 0.8000
Epoch 478/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1753e-05 - acc: 1.0000 - val\_loss: 0.9609 - val\_acc: 0.8000
Epoch 479/500
30/30 [==============================] - 0s 2ms/step - loss: 1.4044e-04 - acc: 1.0000 - val\_loss: 0.9636 - val\_acc: 0.8000
Epoch 480/500
30/30 [==============================] - 0s 2ms/step - loss: 1.7551e-05 - acc: 1.0000 - val\_loss: 0.9661 - val\_acc: 0.8000
Epoch 481/500
30/30 [==============================] - 0s 2ms/step - loss: 1.0684e-05 - acc: 1.0000 - val\_loss: 0.9681 - val\_acc: 0.8000
Epoch 482/500
30/30 [==============================] - 0s 2ms/step - loss: 2.1999e-05 - acc: 1.0000 - val\_loss: 0.9699 - val\_acc: 0.8000
Epoch 483/500
30/30 [==============================] - 0s 2ms/step - loss: 4.1366e-05 - acc: 1.0000 - val\_loss: 0.9713 - val\_acc: 0.8000
Epoch 484/500
30/30 [==============================] - 0s 2ms/step - loss: 9.2909e-05 - acc: 1.0000 - val\_loss: 0.9743 - val\_acc: 0.8000
Epoch 485/500
30/30 [==============================] - 0s 2ms/step - loss: 1.6845e-04 - acc: 1.0000 - val\_loss: 0.9764 - val\_acc: 0.8000
Epoch 486/500
30/30 [==============================] - 0s 2ms/step - loss: 6.2366e-05 - acc: 1.0000 - val\_loss: 0.9781 - val\_acc: 0.8000
Epoch 487/500
30/30 [==============================] - 0s 1ms/step - loss: 8.4907e-05 - acc: 1.0000 - val\_loss: 0.9789 - val\_acc: 0.8000
Epoch 488/500
30/30 [==============================] - 0s 1ms/step - loss: 4.4753e-05 - acc: 1.0000 - val\_loss: 0.9793 - val\_acc: 0.8000
Epoch 489/500
30/30 [==============================] - 0s 2ms/step - loss: 1.8471e-05 - acc: 1.0000 - val\_loss: 0.9796 - val\_acc: 0.8000
Epoch 490/500
30/30 [==============================] - 0s 1ms/step - loss: 1.0246e-04 - acc: 1.0000 - val\_loss: 0.9807 - val\_acc: 0.8000
Epoch 491/500
30/30 [==============================] - 0s 2ms/step - loss: 4.4450e-05 - acc: 1.0000 - val\_loss: 0.9824 - val\_acc: 0.8000
Epoch 492/500
30/30 [==============================] - 0s 1ms/step - loss: 3.5142e-05 - acc: 1.0000 - val\_loss: 0.9841 - val\_acc: 0.8000
Epoch 493/500
30/30 [==============================] - 0s 2ms/step - loss: 5.7185e-05 - acc: 1.0000 - val\_loss: 0.9862 - val\_acc: 0.8000
Epoch 494/500
30/30 [==============================] - 0s 2ms/step - loss: 3.4762e-05 - acc: 1.0000 - val\_loss: 0.9879 - val\_acc: 0.8000
Epoch 495/500
30/30 [==============================] - 0s 2ms/step - loss: 4.3542e-05 - acc: 1.0000 - val\_loss: 0.9892 - val\_acc: 0.8000
Epoch 496/500
30/30 [==============================] - 0s 2ms/step - loss: 1.7977e-05 - acc: 1.0000 - val\_loss: 0.9901 - val\_acc: 0.8000
Epoch 497/500
30/30 [==============================] - 0s 2ms/step - loss: 2.3750e-05 - acc: 1.0000 - val\_loss: 0.9908 - val\_acc: 0.8000
Epoch 498/500
30/30 [==============================] - 0s 2ms/step - loss: 9.7848e-06 - acc: 1.0000 - val\_loss: 0.9913 - val\_acc: 0.8000
Epoch 499/500
30/30 [==============================] - 0s 2ms/step - loss: 2.0198e-05 - acc: 1.0000 - val\_loss: 0.9918 - val\_acc: 0.8000
Epoch 500/500
30/30 [==============================] - 0s 2ms/step - loss: 1.1248e-04 - acc: 1.0000 - val\_loss: 0.9917 - val\_acc: 0.8000

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}42}]:} <keras.engine.sequential.Sequential at 0x1a0807bef28>
\end{Verbatim}
            
    \textbf{Conclusions:} 100\% of accuracy was obtained in the training set
and 80\% in the test set. The model can be improved as follows: - 1.
Acquiring more data - 2. Applying more regularization - 3. Modifying the
optimization algorithm. - 4. Making wise changes in the architecture.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
